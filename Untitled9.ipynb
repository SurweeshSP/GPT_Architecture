{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM8F7NI0W/fbXr6T8gpn96w",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SurweeshSP/GPT_Architecture/blob/main/Untitled9.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "\n",
        "\n",
        "GPT-2 consists of multiple Transformer decoder blocks stacked together. The main components are:\n",
        "\n",
        "(i) Input Embeddings\n",
        "Token Embedding: Converts input tokens into dense vectors.\n",
        "Positional Embedding: Adds positional information since transformers don't have inherent sequential understanding.\n",
        "(ii) Transformer Blocks (Repeated N Times)\n",
        "Each transformer block consists of:\n",
        "\n",
        "Multi-Head Self-Attention (MHSA) – Computes attention scores between all tokens in a sequence.\n",
        "Feedforward Network (FFN) – Applies two dense layers with activation functions.\n",
        "Layer Normalization – Normalizes activations to prevent instability.\n",
        "Dropout – Regularization to avoid overfitting.\n",
        "(iii) Output Layer\n",
        "A final dense layer projects outputs to vocabulary size for token prediction.\n",
        "Softmax converts logits to probabilities.\n",
        "\n",
        "\n",
        "GPT-2 Architecture Diagram\n",
        "mathematica\n",
        "Copy\n",
        "Edit\n",
        "Input Token Sequence → [Token Embedding] + [Positional Embedding]\n",
        "    ↓\n",
        "[Transformer Block 1] → [Transformer Block 2] → ... → [Transformer Block N]\n",
        "    ↓\n",
        "Final Dense Layer → Output Probabilities (Next Token Prediction)\n",
        "Each Transformer Block consists of:\n",
        "\n",
        "css\n",
        "Copy\n",
        "Edit\n",
        "[Input] → [Multi-Head Self Attention] → [Layer Normalization] → [Feed Forward NN] → [Layer Normalization] → [Output]\n",
        "\n",
        "\n",
        "\n",
        "GPT-2 Model Hyperparameters (Different Variants)\n",
        "\n",
        "Model\t      Layers\tHeads\tEmbedding Size\tParameters\n",
        "GPT-2 Small\t   12\t    12\t      768\t          124M\n",
        "GPT-2 Medium   24\t    16\t     1024\t          355M\n",
        "GPT-2 Large\t   36\t    20\t     1280 \t          774M\n",
        "GPT-2 XL\t   48\t    25\t     1600             1.5B'''\n",
        "\n",
        "\n",
        "\n",
        "#Making invalid relationships zero by adding a large negative value: Mask"
      ],
      "metadata": {
        "id": "7iyGgna4tJPr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Layer, Embedding, Dense, LayerNormalization, Dropout\n",
        "from tensorflow.keras import Model\n",
        "\n",
        "# MultiHead Self Attention\n",
        "class MultiHeadSelfAttention(Layer):\n",
        "    def __init__(self, embed_dim, num_heads):\n",
        "        super().__init__()\n",
        "        self.embed_dim = embed_dim\n",
        "        self.num_heads = num_heads\n",
        "        self.att_head_size = embed_dim // num_heads\n",
        "\n",
        "        self.wq = Dense(embed_dim)\n",
        "        self.wk = Dense(embed_dim)\n",
        "        self.wv = Dense(embed_dim)\n",
        "        self.dense = Dense(embed_dim)\n",
        "\n",
        "    def split_heads(self, x, batch_size):\n",
        "        x = tf.reshape(x, (batch_size, -1, self.num_heads, self.att_head_size))\n",
        "        return tf.transpose(x, perm=[0, 2, 1, 3])\n",
        "\n",
        "    def call(self, q, k, v, mask=None):\n",
        "        batch_size = tf.shape(q)[0]\n",
        "\n",
        "        q = self.split_heads(self.wq(q), batch_size)\n",
        "        k = self.split_heads(self.wk(k), batch_size)\n",
        "        v = self.split_heads(self.wv(v), batch_size)\n",
        "\n",
        "        matmul_qk = tf.matmul(q, k, transpose_b=True)\n",
        "        dk = tf.cast(tf.shape(k)[-1], tf.float32)\n",
        "        scaled_attention_logits = matmul_qk / tf.math.sqrt(dk)\n",
        "\n",
        "        if mask is not None:\n",
        "            scaled_attention_logits += (mask * -1e9)\n",
        "\n",
        "        attention_w = tf.nn.softmax(scaled_attention_logits, axis=-1)\n",
        "        output = tf.matmul(attention_w, v)\n",
        "\n",
        "        output = tf.transpose(output, perm=[0, 2, 1, 3])\n",
        "        concat_attention = tf.reshape(output, (batch_size, -1, self.embed_dim))\n",
        "\n",
        "        return self.dense(concat_attention)\n",
        "\n",
        "\n",
        "class FeedForwardNN(Layer):\n",
        "    def __init__(self, embed_dim, dff):\n",
        "        super().__init__()\n",
        "        self.dense1 = Dense(dff, activation='gelu')\n",
        "        self.dense2 = Dense(embed_dim)\n",
        "\n",
        "    def call(self, x):\n",
        "        return self.dense2(self.dense1(x))\n",
        "\n",
        "\n",
        "class Transformer(Layer):\n",
        "    def __init__(self, embed_dim, num_heads, dff, dropout_rate=0.1):\n",
        "        super().__init__()\n",
        "        self.att = MultiHeadSelfAttention(embed_dim, num_heads)\n",
        "        self.ffn = FeedForwardNN(embed_dim, dff)\n",
        "        self.norm1 = LayerNormalization(epsilon=1e-6)\n",
        "        self.norm2 = LayerNormalization(epsilon=1e-6)\n",
        "        self.dropout1 = Dropout(dropout_rate)\n",
        "        self.dropout2 = Dropout(dropout_rate)\n",
        "\n",
        "    def call(self, x, mask=None):\n",
        "        att_output = self.att(x, x, x, mask)\n",
        "        att_output = self.dropout1(att_output)\n",
        "        out1 = self.norm1(x + att_output)\n",
        "\n",
        "        ffn_output = self.ffn(out1)\n",
        "        ffn_output = self.dropout2(ffn_output)\n",
        "\n",
        "        return self.norm2(out1 + ffn_output)\n",
        "\n",
        "\n",
        "class GPT2(Model):\n",
        "    def __init__(self, vocab_size, max_length, embed_dim=768, num_heads=12, dff=3072, num_layers=12, dropout_rate=0.1):\n",
        "        super().__init__()\n",
        "        self.token_emb = Embedding(vocab_size, embed_dim)\n",
        "        self.pos_emb = Embedding(max_length, embed_dim)\n",
        "\n",
        "        self.transformer_blocks = [Transformer(embed_dim, num_heads, dff, dropout_rate) for _ in range(num_layers)]\n",
        "        self.norm = LayerNormalization(epsilon=1e-6)\n",
        "        self.out = Dense(vocab_size)\n",
        "\n",
        "    def create_causal_mask(self, seq_len):\n",
        "        mask = tf.linalg.band_part(tf.ones((seq_len, seq_len)), -1, 0)\n",
        "        return 1 - mask  # Converts upper-triangle values to zero\n",
        "\n",
        "    def call(self, x):\n",
        "        seq_len = tf.shape(x)[1]\n",
        "        mask = self.create_causal_mask(seq_len)\n",
        "\n",
        "        token_emb = self.token_emb(x)\n",
        "        pos_emb = self.pos_emb(tf.range(seq_len)[:, tf.newaxis])\n",
        "        x = token_emb + pos_emb\n",
        "\n",
        "        for transformer in self.transformer_blocks:\n",
        "            x = transformer(x, mask)\n",
        "\n",
        "        x = self.norm(x)\n",
        "        return self.out(x)\n",
        "\n",
        "\n",
        "# Model Initialization\n",
        "VOCAB_SIZE = 3027\n",
        "MAX_LENGTH = 1027\n",
        "\n",
        "inputs = tf.keras.layers.Input(shape=(MAX_LENGTH,), dtype=tf.int32)\n",
        "output = GPT2(vocab_size=VOCAB_SIZE, max_length=MAX_LENGTH)(inputs)\n",
        "gpt2 = Model(inputs, output)\n",
        "\n",
        "gpt2.build(input_shape=(1, MAX_LENGTH))\n",
        "gpt2.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "id": "sh3-n2t8tBBl",
        "outputId": "bdf669d3-2736-4a2b-d358-2c14b2333edf"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"functional\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ input_layer (\u001b[38;5;33mInputLayer\u001b[0m)        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1027\u001b[0m)           │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ gpt2_1 (\u001b[38;5;33mGPT2\u001b[0m)                   │ (\u001b[38;5;34m1027\u001b[0m, \u001b[38;5;34m1027\u001b[0m, \u001b[38;5;34m3027\u001b[0m)     │    \u001b[38;5;34m90,497,235\u001b[0m │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ input_layer (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1027</span>)           │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ gpt2_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GPT2</span>)                   │ (<span style=\"color: #00af00; text-decoration-color: #00af00\">1027</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1027</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3027</span>)     │    <span style=\"color: #00af00; text-decoration-color: #00af00\">90,497,235</span> │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m90,497,235\u001b[0m (345.22 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">90,497,235</span> (345.22 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m90,497,235\u001b[0m (345.22 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">90,497,235</span> (345.22 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install wikipedia"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GL-r01LUreS8",
        "outputId": "0e6ba32a-fd6a-4c9d-fcb2-04fcae4eaeb5"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting wikipedia\n",
            "  Downloading wikipedia-1.4.0.tar.gz (27 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.11/dist-packages (from wikipedia) (4.13.4)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from wikipedia) (2.32.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.0.0->wikipedia) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.0.0->wikipedia) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.0.0->wikipedia) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.0.0->wikipedia) (2025.1.31)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4->wikipedia) (2.6)\n",
            "Requirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4->wikipedia) (4.13.2)\n",
            "Building wheels for collected packages: wikipedia\n",
            "  Building wheel for wikipedia (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for wikipedia: filename=wikipedia-1.4.0-py3-none-any.whl size=11678 sha256=fcef8250f024c30c2c9525007e1b0cdde0397914a878428e093cc9dd5924a542\n",
            "  Stored in directory: /root/.cache/pip/wheels/8f/ab/cb/45ccc40522d3a1c41e1d2ad53b8f33a62f394011ec38cd71c6\n",
            "Successfully built wikipedia\n",
            "Installing collected packages: wikipedia\n",
            "Successfully installed wikipedia-1.4.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 804
        },
        "id": "7ux4iskDrU4J",
        "outputId": "63c89f77-277e-43d5-b6b9-692485f9755f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initializing GPT-2 model...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"gpt2\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"gpt2\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ embedding (\u001b[38;5;33mEmbedding\u001b[0m)           │ (\u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m1024\u001b[0m, \u001b[38;5;34m768\u001b[0m)         │     \u001b[38;5;34m2,304,000\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout (\u001b[38;5;33mDropout\u001b[0m)               │ ?                      │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ transformer (\u001b[38;5;33mTransformer\u001b[0m)       │ ?                      │     \u001b[38;5;34m7,087,872\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ transformer_1 (\u001b[38;5;33mTransformer\u001b[0m)     │ ?                      │     \u001b[38;5;34m7,087,872\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ transformer_2 (\u001b[38;5;33mTransformer\u001b[0m)     │ ?                      │     \u001b[38;5;34m7,087,872\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ transformer_3 (\u001b[38;5;33mTransformer\u001b[0m)     │ ?                      │     \u001b[38;5;34m7,087,872\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ transformer_4 (\u001b[38;5;33mTransformer\u001b[0m)     │ ?                      │     \u001b[38;5;34m7,087,872\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ transformer_5 (\u001b[38;5;33mTransformer\u001b[0m)     │ ?                      │     \u001b[38;5;34m7,087,872\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ transformer_6 (\u001b[38;5;33mTransformer\u001b[0m)     │ ?                      │     \u001b[38;5;34m7,087,872\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ transformer_7 (\u001b[38;5;33mTransformer\u001b[0m)     │ ?                      │     \u001b[38;5;34m7,087,872\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ transformer_8 (\u001b[38;5;33mTransformer\u001b[0m)     │ ?                      │     \u001b[38;5;34m7,087,872\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ transformer_9 (\u001b[38;5;33mTransformer\u001b[0m)     │ ?                      │     \u001b[38;5;34m7,087,872\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ transformer_10 (\u001b[38;5;33mTransformer\u001b[0m)    │ ?                      │     \u001b[38;5;34m7,087,872\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ transformer_11 (\u001b[38;5;33mTransformer\u001b[0m)    │ ?                      │     \u001b[38;5;34m7,087,872\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ layer_normalization_24          │ (\u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m1024\u001b[0m, \u001b[38;5;34m768\u001b[0m)         │         \u001b[38;5;34m1,536\u001b[0m │\n",
              "│ (\u001b[38;5;33mLayerNormalization\u001b[0m)            │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_48 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m1024\u001b[0m, \u001b[38;5;34m3000\u001b[0m)        │     \u001b[38;5;34m2,307,000\u001b[0m │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ embedding (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)           │ (<span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1024</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">768</span>)         │     <span style=\"color: #00af00; text-decoration-color: #00af00\">2,304,000</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)               │ ?                      │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ transformer (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Transformer</span>)       │ ?                      │     <span style=\"color: #00af00; text-decoration-color: #00af00\">7,087,872</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ transformer_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Transformer</span>)     │ ?                      │     <span style=\"color: #00af00; text-decoration-color: #00af00\">7,087,872</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ transformer_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Transformer</span>)     │ ?                      │     <span style=\"color: #00af00; text-decoration-color: #00af00\">7,087,872</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ transformer_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Transformer</span>)     │ ?                      │     <span style=\"color: #00af00; text-decoration-color: #00af00\">7,087,872</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ transformer_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Transformer</span>)     │ ?                      │     <span style=\"color: #00af00; text-decoration-color: #00af00\">7,087,872</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ transformer_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Transformer</span>)     │ ?                      │     <span style=\"color: #00af00; text-decoration-color: #00af00\">7,087,872</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ transformer_6 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Transformer</span>)     │ ?                      │     <span style=\"color: #00af00; text-decoration-color: #00af00\">7,087,872</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ transformer_7 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Transformer</span>)     │ ?                      │     <span style=\"color: #00af00; text-decoration-color: #00af00\">7,087,872</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ transformer_8 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Transformer</span>)     │ ?                      │     <span style=\"color: #00af00; text-decoration-color: #00af00\">7,087,872</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ transformer_9 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Transformer</span>)     │ ?                      │     <span style=\"color: #00af00; text-decoration-color: #00af00\">7,087,872</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ transformer_10 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Transformer</span>)    │ ?                      │     <span style=\"color: #00af00; text-decoration-color: #00af00\">7,087,872</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ transformer_11 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Transformer</span>)    │ ?                      │     <span style=\"color: #00af00; text-decoration-color: #00af00\">7,087,872</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ layer_normalization_24          │ (<span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1024</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">768</span>)         │         <span style=\"color: #00af00; text-decoration-color: #00af00\">1,536</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LayerNormalization</span>)            │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_48 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1024</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3000</span>)        │     <span style=\"color: #00af00; text-decoration-color: #00af00\">2,307,000</span> │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m90,453,432\u001b[0m (345.05 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">90,453,432</span> (345.05 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m90,453,432\u001b[0m (345.05 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">90,453,432</span> (345.05 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fetching Wikipedia content for 'Biology'...\n",
            "Generating text with the new model...\n",
            "=== Generated Output ===\n",
            "biology is the scientific study of life it is a natural science with a broad scope but has several unifying themes that tie it together as a single coherent field for instance all organisms are composed of at least one cell that processes hereditary information encoded in genes which can be transmitted to future generations another major theme is evolution which explains the unity and diversity of life energy processing is also important to life as it allows organisms to move grow and\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "import wikipedia\n",
        "import re\n",
        "import numpy as np\n",
        "from tensorflow.keras.layers import Layer, Embedding, Dense, LayerNormalization, Dropout\n",
        "from tensorflow.keras import Model\n",
        "import random\n",
        "\n",
        "# Constants with better values for optimization\n",
        "VOCAB_SIZE = 3000\n",
        "MAX_LENGTH = 1024\n",
        "EMBED_DIM = 768\n",
        "NUM_HEADS = 12\n",
        "DFF = 3072\n",
        "NUM_LAYERS = 12\n",
        "DROPOUT_RATE = 0.1\n",
        "\n",
        "class MultiHeadSelfAttention(Layer):\n",
        "    def __init__(self, embed_dim, num_heads):\n",
        "        super().__init__()\n",
        "        self.embed_dim = embed_dim\n",
        "        self.num_heads = num_heads\n",
        "\n",
        "        assert embed_dim % num_heads == 0, \"Embedding dimension must be divisible by number of heads\"\n",
        "        self.head_dim = embed_dim // num_heads\n",
        "\n",
        "        self.qkv_proj = Dense(3 * embed_dim)\n",
        "        self.output_proj = Dense(embed_dim)\n",
        "\n",
        "    def call(self, x, mask=None, training=False):\n",
        "        batch_size = tf.shape(x)[0]\n",
        "        seq_len = tf.shape(x)[1]\n",
        "\n",
        "        qkv = self.qkv_proj(x)\n",
        "        qkv = tf.reshape(qkv, [batch_size, seq_len, 3, self.num_heads, self.head_dim])\n",
        "        qkv = tf.transpose(qkv, [2, 0, 3, 1, 4])\n",
        "\n",
        "        q, k, v = qkv[0], qkv[1], qkv[2]\n",
        "\n",
        "        scaled_attention = tf.matmul(q, k, transpose_b=True)\n",
        "        scaled_attention = scaled_attention / tf.math.sqrt(tf.cast(self.head_dim, tf.float32))\n",
        "\n",
        "        if mask is not None:\n",
        "            scaled_attention = tf.where(mask == 0, -1e9, scaled_attention)\n",
        "\n",
        "        attention_weights = tf.nn.softmax(scaled_attention, axis=-1)\n",
        "        output = tf.matmul(attention_weights, v)\n",
        "\n",
        "        output = tf.transpose(output, [0, 2, 1, 3])\n",
        "        output = tf.reshape(output, [batch_size, seq_len, self.embed_dim])\n",
        "\n",
        "        return self.output_proj(output)\n",
        "\n",
        "\n",
        "class FeedForwardNN(Layer):\n",
        "    def __init__(self, embed_dim, dff, dropout_rate=0.1):\n",
        "        super().__init__()\n",
        "        self.dense1 = Dense(dff)\n",
        "        self.dense2 = Dense(embed_dim)\n",
        "        self.dropout = Dropout(dropout_rate)\n",
        "\n",
        "    def call(self, x, training=False):\n",
        "        x = self.dense1(x)\n",
        "        x = tf.nn.gelu(x)\n",
        "        x = self.dropout(x, training=training)\n",
        "        return self.dense2(x)\n",
        "\n",
        "\n",
        "class Transformer(Layer):\n",
        "    def __init__(self, embed_dim, num_heads, dff, dropout_rate=0.1):\n",
        "        super().__init__()\n",
        "        self.att = MultiHeadSelfAttention(embed_dim, num_heads)\n",
        "        self.ffn = FeedForwardNN(embed_dim, dff, dropout_rate)\n",
        "        self.norm1 = LayerNormalization(epsilon=1e-6)\n",
        "        self.norm2 = LayerNormalization(epsilon=1e-6)\n",
        "        self.dropout1 = Dropout(dropout_rate)\n",
        "        self.dropout2 = Dropout(dropout_rate)\n",
        "\n",
        "    def call(self, x, mask=None, training=False):\n",
        "        normalized_x = self.norm1(x)\n",
        "        att_output = self.att(normalized_x, mask, training=training)\n",
        "        att_output = self.dropout1(att_output, training=training)\n",
        "        out1 = x + att_output\n",
        "\n",
        "        normalized_out1 = self.norm2(out1)\n",
        "        ffn_output = self.ffn(normalized_out1, training=training)\n",
        "        ffn_output = self.dropout2(ffn_output, training=training)\n",
        "\n",
        "        return out1 + ffn_output\n",
        "\n",
        "\n",
        "class GPT2(Model):\n",
        "    def __init__(self, vocab_size, max_length, embed_dim=768, num_heads=12, dff=3072, num_layers=12, dropout_rate=0.1):\n",
        "        super().__init__()\n",
        "        self.vocab_size = vocab_size\n",
        "        self.max_length = max_length\n",
        "\n",
        "        self.token_emb = Embedding(vocab_size, embed_dim)\n",
        "        self.pos_emb = self.add_weight(\n",
        "            name=\"positional_embeddings\",\n",
        "            shape=[max_length, embed_dim],\n",
        "            initializer=\"zeros\",\n",
        "            trainable=True\n",
        "        )\n",
        "\n",
        "        self.dropout = Dropout(dropout_rate)\n",
        "        self.transformer_blocks = [\n",
        "            Transformer(embed_dim, num_heads, dff, dropout_rate)\n",
        "            for _ in range(num_layers)\n",
        "        ]\n",
        "        self.norm = LayerNormalization(epsilon=1e-6)\n",
        "        self.out = Dense(vocab_size)\n",
        "\n",
        "        mask = 1 - tf.linalg.band_part(tf.ones((max_length, max_length)), -1, 0)\n",
        "        self.causal_mask = tf.cast(mask, tf.float32)\n",
        "\n",
        "    def call(self, x, training=False):\n",
        "        seq_len = tf.shape(x)[1]\n",
        "        mask = self.causal_mask[:seq_len, :seq_len]\n",
        "\n",
        "        token_emb = self.token_emb(x)\n",
        "        pos_emb = self.pos_emb[:seq_len]\n",
        "        x = token_emb + pos_emb\n",
        "        x = self.dropout(x, training=training)\n",
        "\n",
        "        for transformer in self.transformer_blocks:\n",
        "            x = transformer(x, mask, training=training)\n",
        "\n",
        "        x = self.norm(x)\n",
        "        return self.out(x)\n",
        "\n",
        "    def generate(self, input_ids, max_new_tokens=50, temperature=0.8, top_k=40):\n",
        "        input_ids = tf.convert_to_tensor([input_ids], dtype=tf.int32)\n",
        "\n",
        "        for _ in range(max_new_tokens):\n",
        "            context = input_ids[:, -self.max_length:]\n",
        "            logits = self(context, training=False)\n",
        "            next_token_logits = logits[:, -1, :]\n",
        "            next_token_logits = next_token_logits / temperature\n",
        "\n",
        "            if top_k > 0:\n",
        "                values, _ = tf.math.top_k(next_token_logits, k=top_k)\n",
        "                min_value = values[:, -1]\n",
        "                next_token_logits = tf.where(\n",
        "                    next_token_logits < min_value[:, tf.newaxis],\n",
        "                    tf.ones_like(next_token_logits) * -1e10,\n",
        "                    next_token_logits\n",
        "                )\n",
        "\n",
        "            probs = tf.nn.softmax(next_token_logits, axis=-1)\n",
        "            next_token = tf.random.categorical(tf.math.log(probs), num_samples=1, dtype=tf.int32)\n",
        "            input_ids = tf.concat([input_ids, next_token], axis=1)\n",
        "\n",
        "        return input_ids[0].numpy()\n",
        "\n",
        "\n",
        "class Tokenizer:\n",
        "    def __init__(self, vocab_size=VOCAB_SIZE):\n",
        "        self.vocab_size = vocab_size\n",
        "        self.token2idx = {'<PAD>': 0, '<UNK>': 1, '<EOS>': 2}\n",
        "        self.idx2token = {0: '<PAD>', 1: '<UNK>', 2: '<EOS>'}\n",
        "        self.word_pattern = re.compile(r\"\\b\\w+\\b\")\n",
        "\n",
        "    def fit_on_text(self, text):\n",
        "        text = text.lower()\n",
        "        tokens = self.word_pattern.findall(text)\n",
        "\n",
        "        freq = {}\n",
        "        for token in tokens:\n",
        "            freq[token] = freq.get(token, 0) + 1\n",
        "\n",
        "        sorted_tokens = sorted(freq.items(), key=lambda x: -x[1])\n",
        "        vocab_tokens = [t[0] for t in sorted_tokens[:self.vocab_size - 3]]\n",
        "\n",
        "        for i, token in enumerate(vocab_tokens):\n",
        "            idx = i + 3\n",
        "            self.token2idx[token] = idx\n",
        "            self.idx2token[idx] = token\n",
        "\n",
        "        return self\n",
        "\n",
        "    def encode(self, text, max_length=MAX_LENGTH):\n",
        "        text = text.lower()\n",
        "        tokens = self.word_pattern.findall(text)\n",
        "        ids = [self.token2idx.get(t, 1) for t in tokens[:max_length-1]]\n",
        "        ids.append(2)\n",
        "\n",
        "        if len(ids) < max_length:\n",
        "            ids += [0] * (max_length - len(ids))\n",
        "\n",
        "        return ids\n",
        "\n",
        "    def decode(self, ids):\n",
        "        tokens = []\n",
        "        for id_ in ids:\n",
        "            if id_ == 0 or id_ == 2:\n",
        "                break\n",
        "            tokens.append(self.idx2token.get(id_, '<UNK>'))\n",
        "\n",
        "        return \" \".join(tokens)\n",
        "\n",
        "\n",
        "def fetch_wiki_text(topic, max_retries=3):\n",
        "    for _ in range(max_retries):\n",
        "        try:\n",
        "            page = wikipedia.page(topic, auto_suggest=False)\n",
        "            return page.content\n",
        "        except wikipedia.DisambiguationError as e:\n",
        "            if e.options:\n",
        "                try:\n",
        "                    page = wikipedia.page(e.options[0], auto_suggest=False)\n",
        "                    return page.content\n",
        "                except:\n",
        "                    continue\n",
        "        except Exception as e:\n",
        "            print(f\"Error fetching {topic}: {e}\")\n",
        "            continue\n",
        "\n",
        "    return f\"Unable to fetch content for '{topic}' after {max_retries} attempts.\"\n",
        "\n",
        "\n",
        "def main():\n",
        "    print(\"Initializing GPT-2 model...\")\n",
        "    gpt2 = GPT2(\n",
        "        vocab_size=VOCAB_SIZE,\n",
        "        max_length=MAX_LENGTH,\n",
        "        embed_dim=EMBED_DIM,\n",
        "        num_heads=NUM_HEADS,\n",
        "        dff=DFF,\n",
        "        num_layers=NUM_LAYERS,\n",
        "        dropout_rate=DROPOUT_RATE\n",
        "    )\n",
        "\n",
        "    # Build the model with dummy input to enable model.summary()\n",
        "    dummy_input = tf.constant(np.zeros((1, MAX_LENGTH), dtype=np.int32))\n",
        "    gpt2(dummy_input)  # Trigger build\n",
        "    gpt2.summary()\n",
        "\n",
        "    topic = \"Biology\"\n",
        "    print(f\"Fetching Wikipedia content for '{topic}'...\")\n",
        "\n",
        "    text = fetch_wiki_text(topic)\n",
        "    tokenizer = Tokenizer(vocab_size=VOCAB_SIZE).fit_on_text(text)\n",
        "\n",
        "    input_ids = tokenizer.encode(text[:500])\n",
        "\n",
        "    print(\"Generating text with the new model...\")\n",
        "    output_ids = gpt2.generate(input_ids, max_new_tokens=50, temperature=0.8, top_k=40)\n",
        "    generated_text = tokenizer.decode(output_ids)\n",
        "\n",
        "    print(\"=== Generated Output ===\")\n",
        "    print(generated_text[:800])\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ]
    }
  ]
}